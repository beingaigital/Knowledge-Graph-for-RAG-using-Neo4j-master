#!/usr/bin/env python3
"""
å¢é‡å¤„ç†ç³»ç»Ÿ - è‡ªåŠ¨è¯†åˆ«å·²å¤„ç†æ–‡ä»¶ï¼Œåªå¤„ç†æ–°æ–‡ä»¶
"""

import json
import os
import hashlib
from pathlib import Path
from datetime import datetime
from dotenv import load_dotenv
from langchain_community.graphs import Neo4jGraph
import warnings
warnings.filterwarnings("ignore")

# Load environment variables
load_dotenv('.env', override=True)

# Neo4j connection
NEO4J_URI = os.getenv('NEO4J_URI')
NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')
NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')
NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'

kg = Neo4jGraph(
    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE
)

class IncrementalProcessor:
    def __init__(self):
        self.processed_file = "data/processed_files.json"
        self.chunks_dir = Path("data/chunks")
        self.cleaned_dir = Path("data/cleaned")
        self.json_dir = Path("data/json")
        
    def get_file_hash(self, file_path):
        """è®¡ç®—æ–‡ä»¶MD5å“ˆå¸Œå€¼"""
        hash_md5 = hashlib.md5()
        try:
            with open(file_path, "rb") as f:
                for chunk in iter(lambda: f.read(4096), b""):
                    hash_md5.update(chunk)
            return hash_md5.hexdigest()
        except Exception as e:
            print(f"è®¡ç®—æ–‡ä»¶å“ˆå¸Œå¤±è´¥ {file_path}: {e}")
            return None
    
    def get_file_info(self, file_path):
        """è·å–æ–‡ä»¶ä¿¡æ¯"""
        stat = file_path.stat()
        return {
            "path": str(file_path),
            "name": file_path.name,
            "size": stat.st_size,
            "modified": stat.st_mtime,
            "hash": self.get_file_hash(file_path)
        }
    
    def load_processed_files(self):
        """åŠ è½½å·²å¤„ç†æ–‡ä»¶è®°å½•"""
        if os.path.exists(self.processed_file):
            with open(self.processed_file, 'r', encoding='utf-8') as f:
                return json.load(f)
        return {
            "files": {},
            "chunks": {},
            "last_processed": None
        }
    
    def save_processed_files(self, data):
        """ä¿å­˜å·²å¤„ç†æ–‡ä»¶è®°å½•"""
        os.makedirs(os.path.dirname(self.processed_file), exist_ok=True)
        with open(self.processed_file, 'w', encoding='utf-8') as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
    
    def is_file_processed(self, file_path, processed_data):
        """æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å¤„ç†"""
        file_info = self.get_file_info(file_path)
        file_path_str = str(file_path)
        
        if file_path_str in processed_data["files"]:
            stored_info = processed_data["files"][file_path_str]
            # æ¯”è¾ƒå“ˆå¸Œå€¼ï¼Œå¦‚æœç›¸åŒåˆ™è®¤ä¸ºå·²å¤„ç†
            if stored_info.get("hash") == file_info["hash"]:
                return True
        
        return False
    
    def mark_file_processed(self, file_path, processed_data):
        """æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†"""
        file_info = self.get_file_info(file_path)
        processed_data["files"][str(file_path)] = file_info
        processed_data["last_processed"] = datetime.now().isoformat()
    
    def get_new_files(self, input_dir):
        """è·å–éœ€è¦å¤„ç†çš„æ–°æ–‡ä»¶"""
        processed_data = self.load_processed_files()
        new_files = []
        
        # æ”¯æŒçš„æ–‡ä»¶æ ¼å¼
        supported_extensions = ['.pdf', '.xlsx', '.xls', '.csv', '.docx', '.doc', 
                              '.pptx', '.ppt', '.html', '.htm', '.json', '.txt']
        
        input_path = Path(input_dir)
        if not input_path.exists():
            print(f"è¾“å…¥ç›®å½•ä¸å­˜åœ¨: {input_dir}")
            return new_files
        
        for file_path in input_path.iterdir():
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                if not self.is_file_processed(file_path, processed_data):
                    new_files.append(file_path)
                    print(f"ğŸ†• å‘ç°æ–°æ–‡ä»¶: {file_path.name}")
                else:
                    print(f"â­ï¸  è·³è¿‡å·²å¤„ç†æ–‡ä»¶: {file_path.name}")
        
        return new_files
    
    def process_new_files(self, input_dir="data/raw"):
        """å¤„ç†æ–°æ–‡ä»¶"""
        print("ğŸ”„ å¢é‡å¤„ç†å¼€å§‹")
        print("=" * 60)
        
        # è·å–æ–°æ–‡ä»¶
        new_files = self.get_new_files(input_dir)
        
        if not new_files:
            print("âœ… æ²¡æœ‰æ–°æ–‡ä»¶éœ€è¦å¤„ç†")
            return
        
        print(f"ğŸ“ å‘ç° {len(new_files)} ä¸ªæ–°æ–‡ä»¶éœ€è¦å¤„ç†")
        
        # åŠ è½½å·²å¤„ç†æ–‡ä»¶è®°å½•
        processed_data = self.load_processed_files()
        
        # å¤„ç†æ¯ä¸ªæ–°æ–‡ä»¶
        for file_path in new_files:
            print(f"\nğŸ“„ å¤„ç†æ–‡ä»¶: {file_path.name}")
            
            try:
                # 1. é¢„å¤„ç†
                print("  ğŸ”„ é¢„å¤„ç†...")
                from pr_multi_format_preprocessing import process_multi_format_documents
                process_multi_format_documents(input_dir, "data/cleaned")
                
                # 2. JSONè½¬æ¢
                print("  ğŸ“‹ JSONè½¬æ¢...")
                from pr_txt2json import process_pr_text_files
                process_pr_text_files()
                
                # 3. åˆ†å—
                print("  âœ‚ï¸ åˆ†å—...")
                from pr_chunking import process_all_pr_files
                chunks = process_all_pr_files()
                
                # 4. Neo4jé›†æˆ
                print("  ğŸ”— Neo4jé›†æˆ...")
                from pr_neo4j_simple import main as neo4j_main
                neo4j_main()
                
                # æ ‡è®°æ–‡ä»¶ä¸ºå·²å¤„ç†
                self.mark_file_processed(file_path, processed_data)
                print(f"  âœ… æ–‡ä»¶å¤„ç†å®Œæˆ: {file_path.name}")
                
            except Exception as e:
                print(f"  âŒ å¤„ç†å¤±è´¥: {file_path.name} - {e}")
                continue
        
        # ä¿å­˜å¤„ç†è®°å½•
        self.save_processed_files(processed_data)
        print(f"\nğŸ‰ å¢é‡å¤„ç†å®Œæˆï¼å¤„ç†äº† {len(new_files)} ä¸ªæ–°æ–‡ä»¶")
    
    def check_neo4j_status(self):
        """æ£€æŸ¥Neo4jä¸­çš„èŠ‚ç‚¹çŠ¶æ€"""
        print("\nğŸ“Š æ£€æŸ¥Neo4jçŠ¶æ€...")
        
        try:
            # æ£€æŸ¥PR_ChunkèŠ‚ç‚¹æ•°é‡
            chunk_count_query = "MATCH (c:PR_Chunk) RETURN count(c) as count"
            result = kg.query(chunk_count_query)
            chunk_count = result[0]['count'] if result else 0
            print(f"  PR_ChunkèŠ‚ç‚¹æ•°é‡: {chunk_count}")
            
            # æ£€æŸ¥NEXTå…³ç³»æ•°é‡
            next_count_query = "MATCH ()-[r:NEXT]->() RETURN count(r) as count"
            result = kg.query(next_count_query)
            next_count = result[0]['count'] if result else 0
            print(f"  NEXTå…³ç³»æ•°é‡: {next_count}")
            
            # æ£€æŸ¥å‘é‡ç´¢å¼•
            index_query = "SHOW INDEXES"
            indexes = kg.query(index_query)
            vector_indexes = [idx for idx in indexes if 'vector' in str(idx).lower()]
            print(f"  å‘é‡ç´¢å¼•æ•°é‡: {len(vector_indexes)}")
            
        except Exception as e:
            print(f"  âŒ æ£€æŸ¥Neo4jçŠ¶æ€å¤±è´¥: {e}")
    
    def cleanup_orphaned_chunks(self):
        """æ¸…ç†å­¤ç«‹çš„chunksï¼ˆå¯é€‰åŠŸèƒ½ï¼‰"""
        print("\nğŸ§¹ æ¸…ç†å­¤ç«‹chunks...")
        
        try:
            # æŸ¥æ‰¾æ²¡æœ‰NEXTå…³ç³»çš„chunks
            orphaned_query = """
            MATCH (c:PR_Chunk)
            WHERE NOT (c)-[:NEXT]->() AND NOT ()-[:NEXT]->(c)
            RETURN count(c) as count
            """
            result = kg.query(orphaned_query)
            orphaned_count = result[0]['count'] if result else 0
            
            if orphaned_count > 0:
                print(f"  å‘ç° {orphaned_count} ä¸ªå­¤ç«‹chunks")
                cleanup_choice = input("æ˜¯å¦æ¸…ç†å­¤ç«‹chunks? (y/n): ").strip().lower()
                if cleanup_choice == 'y':
                    delete_query = """
                    MATCH (c:PR_Chunk)
                    WHERE NOT (c)-[:NEXT]->() AND NOT ()-[:NEXT]->(c)
                    DELETE c
                    """
                    kg.query(delete_query)
                    print("  âœ… å­¤ç«‹chunkså·²æ¸…ç†")
            else:
                print("  âœ… æ²¡æœ‰å‘ç°å­¤ç«‹chunks")
                
        except Exception as e:
            print(f"  âŒ æ¸…ç†å¤±è´¥: {e}")
    
    def run(self):
        """è¿è¡Œå¢é‡å¤„ç†å™¨"""
        print("ğŸš€ å¢é‡å¤„ç†ç³»ç»Ÿ")
        print("=" * 60)
        
        # æ£€æŸ¥Neo4jçŠ¶æ€
        self.check_neo4j_status()
        
        # å¤„ç†æ–°æ–‡ä»¶
        self.process_new_files()
        
        # å¯é€‰ï¼šæ¸…ç†å­¤ç«‹chunks
        cleanup_choice = input("\næ˜¯å¦æ£€æŸ¥å¹¶æ¸…ç†å­¤ç«‹chunks? (y/n): ").strip().lower()
        if cleanup_choice == 'y':
            self.cleanup_orphaned_chunks()

if __name__ == "__main__":
    processor = IncrementalProcessor()
    processor.run()


