# SPOä¸‰å…ƒç»„æå–å¹¶ä¸Šä¼ åˆ°Neo4jçš„æ­¥éª¤

## ğŸ“‹ å‰ææ¡ä»¶

1. **ç¡®ä¿ç¯å¢ƒå˜é‡å·²è®¾ç½®**ï¼š
   ```bash
   export OPENAI_API_KEY="ä½ çš„APIå¯†é’¥"
   export NEO4J_URI="bolt://localhost:7687"
   export NEO4J_USERNAME="neo4j"
   export NEO4J_PASSWORD="ä½ çš„å¯†ç "
   export NEO4J_DATABASE="neo4j"
   ```

2. **ç¡®ä¿Neo4jæ•°æ®åº“æ­£åœ¨è¿è¡Œ**

3. **æ•°æ®å·²å‡†å¤‡å¥½**ï¼ˆå·²æœ‰chunksæ–‡ä»¶åœ¨ `data/chunks/` ç›®å½•ä¸‹ï¼‰

---

## ğŸš€ æ­¥éª¤1ï¼šä½¿ç”¨Pythonäº¤äº’å¼ç¯å¢ƒæå–SPOä¸‰å…ƒç»„

åœ¨ç»ˆç«¯ä¸­è¿è¡Œï¼š

```bash
cd /Users/biaowenhuang/Documents/Knowledge-Graph-for-RAG-using-Neo4j-master/pr_rag_v1
python3
```

ç„¶ååœ¨Pythonä¸­æ‰§è¡Œï¼š

```python
import os
import json
from pathlib import Path
from core.pr_spo_extractor import SPOTripleExtractor
from core.pr_kg_builder import KnowledgeGraphBuilder

# è®¾ç½®APIå¯†é’¥
os.environ['OPENAI_API_KEY'] = 'ä½ çš„APIå¯†é’¥'

# åˆå§‹åŒ–SPOæå–å™¨ï¼ˆä½¿ç”¨OpenAIï¼‰
extractor = SPOTripleExtractor(
    model_name="gpt-3.5-turbo",
    use_openrouter=False,
    temperature=0.0
)

# åˆå§‹åŒ–å›¾è°±æ„å»ºå™¨
kg_builder = KnowledgeGraphBuilder()

# è¯»å–æ‰€æœ‰chunksæ–‡ä»¶
chunks_dir = Path("data/chunks")
chunk_files = list(chunks_dir.glob("*_chunks.json"))

print(f"æ‰¾åˆ° {len(chunk_files)} ä¸ªchunksæ–‡ä»¶")

all_triples = []

# å¤„ç†æ¯ä¸ªchunksæ–‡ä»¶
for chunk_file in chunk_files:
    print(f"\nå¤„ç†æ–‡ä»¶: {chunk_file.name}")
    
    with open(chunk_file, 'r', encoding='utf-8') as f:
        chunks_data = json.load(f)
    
    for i, chunk in enumerate(chunks_data):
        chunk_text = chunk.get('text', '')
        if not chunk_text:
            continue
            
        print(f"  å¤„ç†chunk {i+1}/{len(chunks_data)}...")
        
        # æå–SPOä¸‰å…ƒç»„
        result = extractor.extract_triples_from_text(
            chunk_text,
            chunk_size=150,
            overlap=30,
            verbose=False
        )
        
        # å½’ä¸€åŒ–ä¸‰å…ƒç»„
        normalized = extractor.normalize_triples(result['triples'])
        
        # æ·»åŠ æ¥æºä¿¡æ¯
        for triple in normalized:
            triple['source_file'] = chunk_file.name
            triple['chunk_id'] = chunk.get('chunkId', f"chunk_{i}")
        
        all_triples.extend(normalized)
        
        # æ·»åŠ åˆ°å›¾è°±
        kg_builder.add_triples(normalized)
        
        print(f"    æå–äº† {len(normalized)} ä¸ªä¸‰å…ƒç»„")

print(f"\nâœ… æ€»å…±æå–äº† {len(all_triples)} ä¸ªä¸‰å…ƒç»„")

# ä¿å­˜ä¸‰å…ƒç»„åˆ°æ–‡ä»¶ï¼ˆå¯é€‰ï¼‰
with open('data/spo_triples.json', 'w', encoding='utf-8') as f:
    json.dump(all_triples, f, ensure_ascii=False, indent=2)
print("âœ… ä¸‰å…ƒç»„å·²ä¿å­˜åˆ° data/spo_triples.json")

# æ˜¾ç¤ºå›¾è°±ç»Ÿè®¡
stats = kg_builder.get_statistics()
print(f"\nğŸ“Š å›¾è°±ç»Ÿè®¡:")
print(f"  èŠ‚ç‚¹æ•°: {stats['nodes']}")
print(f"  è¾¹æ•°: {stats['edges']}")
```

---

## ğŸ”— æ­¥éª¤2ï¼šå°†SPOä¸‰å…ƒç»„ä¸Šä¼ åˆ°Neo4j

ç»§ç»­åœ¨Pythonä¸­æ‰§è¡Œï¼š

```python
from langchain_community.graphs import Neo4jGraph
from dotenv import load_dotenv
load_dotenv()

# è¿æ¥Neo4j
neo4j_uri = os.getenv('NEO4J_URI')
neo4j_user = os.getenv('NEO4J_USERNAME')
neo4j_pwd = os.getenv('NEO4J_PASSWORD')
neo4j_db = os.getenv('NEO4J_DATABASE') or 'neo4j'

graph = Neo4jGraph(
    url=neo4j_uri,
    username=neo4j_user,
    password=neo4j_pwd,
    database=neo4j_db
)

print("âœ… Neo4jè¿æ¥æˆåŠŸ")

# åˆ›å»ºç´¢å¼•
print("\nğŸ”§ åˆ›å»ºç´¢å¼•...")
index_queries = [
    "CREATE INDEX IF NOT EXISTS FOR (n:Entity) ON (n.name)",
    "CREATE INDEX IF NOT EXISTS FOR (n:PR_Chunk) ON (n.chunk_id)",
    "CREATE INDEX IF NOT EXISTS FOR ()-[r:HAS_RELATION]-() ON (r.predicate)"
]

for query in index_queries:
    try:
        graph.query(query)
    except Exception as e:
        print(f"âš ï¸ ç´¢å¼•åˆ›å»ºè­¦å‘Š: {e}")

# ä¸Šä¼ ä¸‰å…ƒç»„
print(f"\nğŸ“¤ å¼€å§‹ä¸Šä¼  {len(all_triples)} ä¸ªä¸‰å…ƒç»„åˆ°Neo4j...")

success_count = 0
for i, triple in enumerate(all_triples, 1):
    subject = triple['subject']
    predicate = triple['predicate']
    obj = triple['object']
    chunk_id = triple.get('chunk_id', '')
    source_file = triple.get('source_file', '')
    
    try:
        # åˆ›å»ºå®ä½“èŠ‚ç‚¹å’Œå…³ç³»
        cypher = """
        MERGE (s:Entity {name: $subject})
        MERGE (o:Entity {name: $object})
        MERGE (s)-[r:HAS_RELATION {predicate: $predicate}]->(o)
        ON CREATE SET 
            r.created_at = timestamp(),
            r.source_file = $source_file,
            r.chunk_id = $chunk_id
        ON MATCH SET
            r.updated_at = timestamp()
        
        // å¦‚æœchunk_idå­˜åœ¨ï¼Œå…³è”åˆ°chunkèŠ‚ç‚¹
        WITH s, o, r
        WHERE $chunk_id <> ''
        MERGE (chunk:PR_Chunk {chunk_id: $chunk_id})
        MERGE (chunk)-[:CONTAINS_ENTITY]->(s)
        MERGE (chunk)-[:CONTAINS_ENTITY]->(o)
        """
        
        graph.query(cypher, params={
            'subject': subject,
            'object': obj,
            'predicate': predicate,
            'chunk_id': chunk_id,
            'source_file': source_file
        })
        
        success_count += 1
        
        if i % 50 == 0:
            print(f"  è¿›åº¦: {i}/{len(all_triples)} ({i/len(all_triples)*100:.1f}%)")
            
    except Exception as e:
        print(f"âš ï¸ ä¸Šä¼ ä¸‰å…ƒç»„ {i} å¤±è´¥: {e}")

print(f"\nâœ… æˆåŠŸä¸Šä¼  {success_count}/{len(all_triples)} ä¸ªä¸‰å…ƒç»„")

# æŸ¥è¯¢ç»Ÿè®¡
stats_query = """
MATCH (n:Entity)
RETURN count(n) as entity_count
UNION ALL
MATCH ()-[r:HAS_RELATION]->()
RETURN count(r) as relation_count
"""
result = graph.query(stats_query)
print(f"\nğŸ“Š Neo4jç»Ÿè®¡:")
for row in result:
    if 'entity_count' in row:
        print(f"  å®ä½“èŠ‚ç‚¹: {row['entity_count']}")
    elif 'relation_count' in row:
        print(f"  å…³ç³»æ•°é‡: {row['relation_count']}")
```

---

## ğŸ¯ æˆ–è€…ä½¿ç”¨æ›´ç®€å•çš„æ–¹æ³•ï¼šä¿®æ”¹ç°æœ‰ä¸Šä¼ è„šæœ¬

å¦‚æœä½ æƒ³ä¸€æ¬¡æ€§å®Œæˆï¼Œå¯ä»¥ï¼š

**æ–¹æ³•1ï¼šç›´æ¥åœ¨ç»ˆç«¯è¿è¡ŒPythonè„šæœ¬**

åˆ›å»ºä¸€ä¸ªä¸´æ—¶è„šæœ¬ `temp_upload_spo.py`ï¼š

```python
#!/usr/bin/env python3
import os
import json
from pathlib import Path
from core.pr_spo_extractor import SPOTripleExtractor
from langchain_community.graphs import Neo4jGraph
from dotenv import load_dotenv

load_dotenv()

# è®¾ç½®APIå¯†é’¥
os.environ['OPENAI_API_KEY'] = 'ä½ çš„APIå¯†é’¥'

# åˆå§‹åŒ–
extractor = SPOTripleExtractor(model_name="gpt-3.5-turbo", use_openrouter=False)
graph = Neo4jGraph(
    url=os.getenv('NEO4J_URI'),
    username=os.getenv('NEO4J_USERNAME'),
    password=os.getenv('NEO4J_PASSWORD'),
    database=os.getenv('NEO4J_DATABASE') or 'neo4j'
)

# å¤„ç†æ‰€æœ‰chunksæ–‡ä»¶
chunks_dir = Path("data/chunks")
all_triples = []

for chunk_file in chunks_dir.glob("*_chunks.json"):
    print(f"å¤„ç†: {chunk_file.name}")
    with open(chunk_file, 'r', encoding='utf-8') as f:
        chunks_data = json.load(f)
    
    for chunk in chunks_data:
        text = chunk.get('text', '')
        if text:
            result = extractor.extract_triples_from_text(text, verbose=False)
            normalized = extractor.normalize_triples(result['triples'])
            all_triples.extend(normalized)

# ä¸Šä¼ åˆ°Neo4j
for triple in all_triples:
    graph.query("""
    MERGE (s:Entity {name: $s})
    MERGE (o:Entity {name: $o})
    MERGE (s)-[r:HAS_RELATION {predicate: $p}]->(o)
    """, params={'s': triple['subject'], 'o': triple['object'], 'p': triple['predicate']})

print(f"âœ… å®Œæˆï¼ä¸Šä¼ äº† {len(all_triples)} ä¸ªä¸‰å…ƒç»„")
```

ç„¶åè¿è¡Œï¼š
```bash
python3 temp_upload_spo.py
```

---

## âš¡ å¿«é€Ÿå‘½ä»¤ï¼ˆå¦‚æœæ•°æ®é‡ä¸å¤§ï¼‰

å¦‚æœä½ åªæƒ³å¿«é€Ÿæµ‹è¯•ï¼Œå¯ä»¥ç”¨è¿™ä¸ªä¸€è¡Œå¼å‘½ä»¤ï¼š

```bash
python3 -c "
import os, json
from pathlib import Path
from core.pr_spo_extractor import SPOTripleExtractor
os.environ['OPENAI_API_KEY'] = 'ä½ çš„APIå¯†é’¥'
extractor = SPOTripleExtractor(model_name='gpt-3.5-turbo', use_openrouter=False)
chunks_file = Path('data/chunks/2025å†…å®¹è¥é”€é‡ç‚¹ç­–ç•¥ä¸æ¡ˆä¾‹_chunks.json')
with open(chunks_file) as f:
    data = json.load(f)
for chunk in data[:3]:  # åªå¤„ç†å‰3ä¸ª
    result = extractor.extract_triples_from_text(chunk['text'], verbose=False)
    print(f'æå–äº† {len(result[\"triples\"])} ä¸ªä¸‰å…ƒç»„')
"
```

---

## ğŸ“ æ³¨æ„äº‹é¡¹

1. **APIè°ƒç”¨è´¹ç”¨**ï¼šSPOæå–éœ€è¦è°ƒç”¨OpenAI APIï¼Œä¼šäº§ç”Ÿè´¹ç”¨
2. **å¤„ç†æ—¶é—´**ï¼šå¤§é‡chunkséœ€è¦è¾ƒé•¿æ—¶é—´
3. **å»ºè®®åˆ†æ‰¹å¤„ç†**ï¼šå¯ä»¥å…ˆå¤„ç†1-2ä¸ªæ–‡ä»¶æµ‹è¯•ï¼Œç¡®è®¤æ— è¯¯åå†å¤„ç†å…¨éƒ¨
4. **Neo4jè¿æ¥**ï¼šç¡®ä¿Neo4jæ•°æ®åº“æ­£åœ¨è¿è¡Œ

---

## ğŸ‰ å®Œæˆåçš„éªŒè¯

ä¸Šä¼ å®Œæˆåï¼Œå¯ä»¥åœ¨Neo4j Browserä¸­æŸ¥è¯¢ï¼š

```cypher
// æŸ¥çœ‹æ‰€æœ‰å®ä½“
MATCH (n:Entity) RETURN n LIMIT 100

// æŸ¥çœ‹å…³ç³»
MATCH ()-[r:HAS_RELATION]->() RETURN r LIMIT 50

// ç»Ÿè®¡
MATCH (n:Entity) RETURN count(n) as entities
MATCH ()-[r:HAS_RELATION]->() RETURN count(r) as relations
```


