#!/usr/bin/env python3
"""
åŸºäºå›¾è°±çš„RAGæŸ¥è¯¢æ¨¡å—
ä½¿ç”¨çŸ¥è¯†å›¾è°±è¿›è¡Œæ£€ç´¢å¢å¼ºç”Ÿæˆ
"""

import os
from typing import List, Dict, Any, Set, Optional
import warnings
warnings.filterwarnings('ignore')

try:
    import openai
except ImportError:
    print("âš ï¸ è­¦å‘Š: openaiåº“æœªå®‰è£…ï¼Œè¯·è¿è¡Œ: pip install openai")
    openai = None

from pr_kg_builder import KnowledgeGraphBuilder


class KnowledgeGraphRAG:
    """åŸºäºçŸ¥è¯†å›¾è°±çš„RAGç³»ç»Ÿ"""
    
    def __init__(
        self,
        knowledge_graph: KnowledgeGraphBuilder,
        model_name: str = "deepseek/deepseek-chat-v3-0324",
        api_key: Optional[str] = None,
        base_url: Optional[str] = None,
        temperature: float = 0.7,
        max_tokens: int = 500,
        use_openrouter: bool = True
    ):
        """
        åˆå§‹åŒ–å›¾è°±RAGç³»ç»Ÿ
        
        Args:
            knowledge_graph: çŸ¥è¯†å›¾è°±æ„å»ºå™¨å®ä¾‹
            model_name: LLMæ¨¡å‹åç§°
            api_key: APIå¯†é’¥
            base_url: APIåŸºç¡€URL
            temperature: ç”Ÿæˆæ¸©åº¦
            max_tokens: æœ€å¤§tokenæ•°
            use_openrouter: æ˜¯å¦ä½¿ç”¨OpenRouter
        """
        self.kg = knowledge_graph
        
        # é…ç½®API
        if use_openrouter:
            self.api_key = api_key or os.getenv("OPENROUTER_API_KEY")
            self.base_url = base_url or "https://openrouter.ai/api/v1"
        else:
            self.api_key = api_key or os.getenv("OPENAI_API_KEY")
            self.base_url = base_url or None
        
        self.model_name = model_name if model_name != "deepseek/deepseek-chat-v3-0324" else "gpt-3.5-turbo"
        self.temperature = temperature
        self.max_tokens = max_tokens
        
        # åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯
        if not self.api_key:
            raise ValueError(
                "API keyæœªè®¾ç½®ã€‚è¯·è®¾ç½®ç¯å¢ƒå˜é‡ OPENROUTER_API_KEY æˆ– OPENAI_API_KEY"
            )
        
        try:
            self.client = openai.OpenAI(
                base_url=self.base_url,
                api_key=self.api_key
            )
        except Exception as e:
            raise Exception(f"OpenAIå®¢æˆ·ç«¯åˆå§‹åŒ–å¤±è´¥: {e}")
    
    def _extract_entities_from_question(
        self,
        question: str,
        normalized_triples: List[Dict[str, Any]]
    ) -> Set[str]:
        """
        ä»é—®é¢˜ä¸­æå–ç›¸å…³å®ä½“
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            normalized_triples: å½’ä¸€åŒ–çš„ä¸‰å…ƒç»„åˆ—è¡¨ï¼ˆç”¨äºå®ä½“åŒ¹é…ï¼‰
            
        Returns:
            ç›¸å…³å®ä½“é›†åˆ
        """
        question_lower = question.lower()
        question_words = set(question_lower.split())
        
        query_entities = set()
        
        # ä»ä¸‰å…ƒç»„ä¸­åŒ¹é…å®ä½“
        for triple in normalized_triples:
            subject = triple.get('subject', '')
            obj = triple.get('object', '')
            
            # æ£€æŸ¥subjectæ˜¯å¦åœ¨é—®é¢˜ä¸­
            if subject in question_lower or any(
                word in question_words for word in subject.split()
            ):
                query_entities.add(subject)
            
            # æ£€æŸ¥objectæ˜¯å¦åœ¨é—®é¢˜ä¸­
            if obj in question_lower or any(
                word in question_words for word in obj.split()
            ):
                query_entities.add(obj)
        
        return query_entities
    
    def _build_context_from_graph(
        self,
        entities: Set[str],
        max_triples: int = 50
    ) -> str:
        """
        ä»å›¾è°±ä¸­æ„å»ºä¸Šä¸‹æ–‡
        
        Args:
            entities: ç›¸å…³å®ä½“é›†åˆ
            max_triples: æœ€å¤§ä¸‰å…ƒç»„æ•°
            
        Returns:
            ä¸Šä¸‹æ–‡æ–‡æœ¬
        """
        if not entities:
            return "æœªæ‰¾åˆ°ç›¸å…³é—®é¢˜å®ä½“ã€‚"
        
        # è·å–ç›¸å…³çš„ä¸‰å…ƒç»„æ–‡æœ¬
        triples_text = self.kg.get_triples_for_context(entities, max_edges=max_triples)
        
        if not triples_text:
            return "å›¾è°±ä¸­æœªæ‰¾åˆ°ä¸é—®é¢˜ç›¸å…³çš„ä¿¡æ¯ã€‚"
        
        context = "çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡:\n"
        context += "\n".join(triples_text)
        
        return context
    
    def query(
        self,
        question: str,
        normalized_triples: List[Dict[str, Any]],
        max_context_triples: int = 50,
        verbose: bool = False
    ) -> str:
        """
        ä½¿ç”¨å›¾è°±RAGå›ç­”é—®é¢˜
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            normalized_triples: å½’ä¸€åŒ–çš„ä¸‰å…ƒç»„åˆ—è¡¨
            max_context_triples: æœ€å¤§ä¸Šä¸‹æ–‡ä¸‰å…ƒç»„æ•°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            å›ç­”
        """
        # 1. ä»é—®é¢˜ä¸­æå–å®ä½“
        query_entities = self._extract_entities_from_question(question, normalized_triples)
        
        if verbose:
            print(f"ğŸ” è¯†åˆ«çš„é—®é¢˜å®ä½“: {list(query_entities)}")
        
        # 2. ä»å›¾è°±ä¸­æå–å­å›¾
        relevant_subgraph = self.kg.get_subgraph_by_entities(query_entities, max_depth=2)
        
        if verbose:
            print(f"ğŸ“Š æå–çš„å­å›¾: {relevant_subgraph.number_of_nodes()} èŠ‚ç‚¹, "
                  f"{relevant_subgraph.number_of_edges()} è¾¹")
        
        # 3. æ„å»ºä¸Šä¸‹æ–‡
        context_text = self._build_context_from_graph(query_entities, max_context_triples)
        
        if verbose:
            print(f"\nğŸ“ ä¸Šä¸‹æ–‡æ–‡æœ¬:\n{context_text[:500]}...")
        
        # 4. ç”Ÿæˆå›ç­”
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”ä¸“å®¶ï¼Œæ“…é•¿åŸºäºçŸ¥è¯†å›¾è°±å›ç­”é—®é¢˜ã€‚
è¯·ä½¿ç”¨æä¾›çš„çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡æ¥ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­ä¸åŒ…å«ç­”æ¡ˆï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

é—®é¢˜: {question}

ä¸Šä¸‹æ–‡:
{context_text}

ç®€æ´å›ç­”:
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            answer = response.choices[0].message.content.strip()
            return answer
        
        except Exception as e:
            return f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}"
    
    def query_with_related_entities(
        self,
        question: str,
        normalized_triples: List[Dict[str, Any]],
        max_hops: int = 2,
        max_context_triples: int = 50,
        verbose: bool = False
    ) -> Dict[str, Any]:
        """
        ä½¿ç”¨ç›¸å…³å®ä½“å¢å¼ºçš„æŸ¥è¯¢
        
        Args:
            question: ç”¨æˆ·é—®é¢˜
            normalized_triples: å½’ä¸€åŒ–çš„ä¸‰å…ƒç»„åˆ—è¡¨
            max_hops: æœ€å¤§è·³æ•°
            max_context_triples: æœ€å¤§ä¸Šä¸‹æ–‡ä¸‰å…ƒç»„æ•°
            verbose: æ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯
            
        Returns:
            åŒ…å«ç­”æ¡ˆå’Œå…ƒæ•°æ®çš„å­—å…¸
        """
        # æå–åˆå§‹å®ä½“
        initial_entities = self._extract_entities_from_question(question, normalized_triples)
        
        # æ‰©å±•ç›¸å…³å®ä½“
        all_entities = set(initial_entities)
        for entity in initial_entities:
            related = self.kg.find_related_entities(entity, max_hops=max_hops)
            for rel in related:
                all_entities.add(rel['entity'])
        
        if verbose:
            print(f"ğŸ” åˆå§‹å®ä½“: {list(initial_entities)}")
            print(f"ğŸ”— æ‰©å±•åçš„å®ä½“: {len(all_entities)} ä¸ª")
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context_text = self._build_context_from_graph(all_entities, max_context_triples)
        
        # ç”Ÿæˆå›ç­”
        prompt = f"""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„é—®ç­”ä¸“å®¶ï¼Œæ“…é•¿åŸºäºçŸ¥è¯†å›¾è°±å›ç­”é—®é¢˜ã€‚
è¯·ä½¿ç”¨æä¾›çš„çŸ¥è¯†å›¾è°±ä¸Šä¸‹æ–‡æ¥ç®€æ´åœ°å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
å¦‚æœä¸Šä¸‹æ–‡ä¸­ä¸åŒ…å«ç­”æ¡ˆï¼Œè¯·è¯´"æˆ‘ä¸çŸ¥é“"ã€‚

é—®é¢˜: {question}

ä¸Šä¸‹æ–‡:
{context_text}

ç®€æ´å›ç­”:
"""
        
        try:
            response = self.client.chat.completions.create(
                model=self.model_name,
                messages=[
                    {"role": "user", "content": prompt}
                ],
                temperature=self.temperature,
                max_tokens=self.max_tokens
            )
            
            answer = response.choices[0].message.content.strip()
            
            return {
                'answer': answer,
                'entities_used': list(all_entities),
                'context_triples_count': len(context_text.split('\n')) - 1,
                'initial_entities': list(initial_entities)
            }
        
        except Exception as e:
            return {
                'answer': f"âŒ æŸ¥è¯¢å¤±è´¥: {str(e)}",
                'entities_used': [],
                'context_triples_count': 0,
                'initial_entities': []
            }


def test_kg_rag():
    """æµ‹è¯•å›¾è°±RAG"""
    print("ğŸ§ª æµ‹è¯•åŸºäºå›¾è°±çš„RAG")
    print("=" * 60)
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    test_triples = [
        {'subject': 'marie curie', 'predicate': 'discovered', 'object': 'radium'},
        {'subject': 'marie curie', 'predicate': 'won', 'object': 'nobel prize in physics'},
        {'subject': 'marie curie', 'predicate': 'won', 'object': 'nobel prize in chemistry'},
        {'subject': 'marie curie', 'predicate': 'married', 'object': 'pierre curie'},
        {'subject': 'pierre curie', 'predicate': 'had children', 'object': 'irene curie'},
        {'subject': 'pierre curie', 'predicate': 'had children', 'object': 'eve curie'},
        {'subject': 'marie curie', 'predicate': 'was born', 'object': '1867'},
        {'subject': 'marie curie', 'predicate': 'died', 'object': '1934'},
    ]
    
    # æ„å»ºå›¾è°±
    from pr_kg_builder import KnowledgeGraphBuilder
    kg = KnowledgeGraphBuilder()
    kg.add_triples(test_triples)
    print(f"âœ… å›¾è°±æ„å»ºå®Œæˆ: {kg.get_statistics()['nodes']} èŠ‚ç‚¹, {kg.get_statistics()['edges']} è¾¹")
    
    # åˆ›å»ºRAGç³»ç»Ÿ
    try:
        rag = KnowledgeGraphRAG(kg, verbose=True)
        
        # æµ‹è¯•æŸ¥è¯¢
        questions = [
            "ç›ä¸½Â·å±…é‡Œåœ¨å“ªä¸¤ä¸ªé¢†åŸŸè·å¾—äº†è¯ºè´å°”å¥–ï¼Ÿ",
            "çš®åŸƒå°”Â·å±…é‡Œçš„å­©å­æ˜¯è°ï¼Ÿ",
            "ç›ä¸½Â·å±…é‡Œå»ä¸–æ—¶å¤šå°‘å²ï¼Ÿ"
        ]
        
        print("\n" + "=" * 60)
        for q in questions:
            print(f"\nâ“ é—®é¢˜: {q}")
            answer = rag.query(q, test_triples, verbose=False)
            print(f"âœ… å›ç­”: {answer}")
            print("-" * 60)
    
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        print("   æç¤º: è¯·è®¾ç½® OPENROUTER_API_KEY æˆ– OPENAI_API_KEY ç¯å¢ƒå˜é‡")


if __name__ == "__main__":
    test_kg_rag()

