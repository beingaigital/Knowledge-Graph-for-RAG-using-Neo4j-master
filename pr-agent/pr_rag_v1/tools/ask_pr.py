#!/usr/bin/env python3
"""
å…¬å…³ä¼ æ’­RAGé—®ç­”ç³»ç»Ÿ
ç›´æ¥ä»Neo4jæŸ¥è¯¢ï¼Œè·³è¿‡æ‰€æœ‰é¢„å¤„ç†æ­¥éª¤
"""

import sys
import os
import textwrap
from dotenv import load_dotenv
from langchain_community.graphs import Neo4jGraph
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import Neo4jVector
from langchain.prompts import PromptTemplate
from langchain.chains import LLMChain
import warnings
warnings.filterwarnings("ignore")

# Load environment variables
load_dotenv('.env', override=True)

# Neo4j connection
NEO4J_URI = os.getenv('NEO4J_URI')
NEO4J_USERNAME = os.getenv('NEO4J_USERNAME')
NEO4J_PASSWORD = os.getenv('NEO4J_PASSWORD')
NEO4J_DATABASE = os.getenv('NEO4J_DATABASE') or 'neo4j'

# Initialize connections
kg = Neo4jGraph(
    url=NEO4J_URI, username=NEO4J_USERNAME, password=NEO4J_PASSWORD, database=NEO4J_DATABASE
)

llm = ChatOpenAI(temperature=0)
embeddings = OpenAIEmbeddings()

def ask_question(question):
    """è¯¢é—®é—®é¢˜å¹¶è·å–å›ç­”"""
    print(f"ğŸ¤” é—®é¢˜: {question}")
    print("=" * 80)
    
    try:
        # ä½¿ç”¨å‘é‡æœç´¢
        vector_store = Neo4jVector.from_existing_graph(
            embedding=embeddings,
            url=NEO4J_URI,
            username=NEO4J_USERNAME,
            password=NEO4J_PASSWORD,
            index_name='PR_OpenAI',
            node_label='PR_Chunk',
            text_node_properties=['text'],
            embedding_node_property='textEmbeddingOpenAI',
        )
        
        # æœç´¢ç›¸å…³æ–‡æ¡£
        docs = vector_store.similarity_search(question, k=5)
        
        if not docs:
            return "âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯ï¼Œè¯·æ£€æŸ¥Neo4jæ•°æ®åº“ä¸­æ˜¯å¦æœ‰PR_ChunkèŠ‚ç‚¹"
        
        print(f"ğŸ“š æ‰¾åˆ° {len(docs)} ä¸ªç›¸å…³æ–‡æ¡£ç‰‡æ®µ")
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = "\n\n".join([doc.page_content for doc in docs])
        
        # åˆ›å»ºä¸“ä¸šprompt
        prompt_template = """
        ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å…³ä¼ æ’­å’Œå“ç‰Œè¥é”€ä¸“å®¶ã€‚åŸºäºä»¥ä¸‹å…¬å…³ä¼ æ’­ç›¸å…³å†…å®¹ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚
        
        ç›¸å…³å†…å®¹ï¼š
        {context}
        
        é—®é¢˜ï¼š{question}
        
        è¯·åŸºäºè¿™äº›å†…å®¹ï¼Œç»™å‡ºä¸“ä¸šã€è¯¦ç»†ã€å®ç”¨çš„å›ç­”ã€‚å›ç­”è¦æ±‚ï¼š
        1. åŸºäºæä¾›çš„å…·ä½“å†…å®¹è¿›è¡Œå›ç­”
        2. ç»“åˆå…¬å…³ä¼ æ’­å’Œå“ç‰Œè¥é”€çš„ä¸“ä¸šçŸ¥è¯†
        3. æä¾›å…·ä½“å¯è¡Œçš„å»ºè®®å’Œç­–ç•¥
        4. å›ç­”è¦ä¸“ä¸šã€å‡†ç¡®ã€æœ‰é’ˆå¯¹æ€§
        5. å¦‚æœå†…å®¹ä¸å¤Ÿå……åˆ†ï¼Œå¯ä»¥ç»“åˆä½ çš„ä¸“ä¸šçŸ¥è¯†è¿›è¡Œè¡¥å……
        
        è¯·ç”¨ä¸­æ–‡å›ç­”ã€‚
        """
        
        prompt = PromptTemplate(
            input_variables=["context", "question"],
            template=prompt_template
        )
        
        chain = LLMChain(llm=llm, prompt=prompt)
        response = chain.run(context=context, question=question)
        
        return response
        
    except Exception as e:
        return f"âŒ æŸ¥è¯¢å¤±è´¥: {e}"

def main():
    """ä¸»å‡½æ•°"""
    if len(sys.argv) < 2:
        print("ğŸš€ å…¬å…³ä¼ æ’­RAGé—®ç­”ç³»ç»Ÿ")
        print("=" * 60)
        print("ä½¿ç”¨æ–¹æ³•:")
        print("  python3 ask_pr.py 'ä½ çš„é—®é¢˜'")
        print("\nç¤ºä¾‹:")
        print("  python3 ask_pr.py 'ç¾å¦†ç±»å“ç‰Œåº”è¯¥å¦‚ä½•å»ºç«‹å’Œæ¶ˆè´¹è€…çš„è”ç³»'")
        print("  python3 ask_pr.py 'åä¸åæœ‰å“ªäº›æˆåŠŸçš„å“ç‰Œæ¡ˆä¾‹'")
        print("  python3 ask_pr.py 'å†…å®¹è¥é”€çš„æ ¸å¿ƒç­–ç•¥æ˜¯ä»€ä¹ˆ'")
        return
    
    # è·å–é—®é¢˜
    question = " ".join(sys.argv[1:])
    
    # è¯¢é—®é—®é¢˜
    answer = ask_question(question)
    
    # æ˜¾ç¤ºå›ç­”
    print(f"\nğŸ¤– å›ç­”:")
    print("-" * 40)
    print(textwrap.fill(answer, 80))

if __name__ == "__main__":
    main()


