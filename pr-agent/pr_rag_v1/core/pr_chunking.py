#!/usr/bin/env python3
"""
å…¬å…³ä¼ æ’­å†…å®¹åˆ†å—è„šæœ¬
å°†JSONæ•°æ®åˆ†å‰²æˆé€‚åˆRAGå¤„ç†çš„å°å—
"""

import json
import os
from langchain.text_splitter import RecursiveCharacterTextSplitter
from pathlib import Path

# é…ç½®æ–‡æœ¬åˆ†å‰²å™¨ï¼Œé€‚é…å…¬å…³ä¼ æ’­å†…å®¹
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=2000,  # é€‚åˆå…¬å…³å†…å®¹çš„å—å¤§å°
    chunk_overlap=200,  # é‡å éƒ¨åˆ†ï¼Œä¿æŒä¸Šä¸‹æ–‡è¿è´¯æ€§
    length_function=len,
    separators=["\n\n", "\n", "ã€‚", "ï¼", "ï¼Ÿ", "ï¼›", " ", ""]
)

def split_pr_data_from_file(file):
    """åˆ†å‰²å…¬å…³ä¼ æ’­æ•°æ®æ–‡ä»¶"""
    chunks_with_metadata = []
    
    try:
        file_as_object = json.load(open(file, 'r', encoding='utf-8'))
        keys = list(file_as_object.keys())
        print(f"Processing file: {file}")
        print(f"Found sections: {keys}")
        
        for item in keys:
            print(f'Processing {item} from {file}')
            item_text = file_as_object[item]
            
            # å¤„ç†ä¸åŒç±»å‹çš„æ•°æ®
            if isinstance(item_text, list):
                # å¦‚æœæ˜¯åˆ—è¡¨ï¼Œç”¨æ¢è¡Œç¬¦è¿æ¥
                item_text = '\n'.join(str(x) for x in item_text)
            elif not isinstance(item_text, str):
                # è½¬æ¢ä¸ºå­—ç¬¦ä¸²
                item_text = str(item_text)
            
            # åˆ†å‰²æ–‡æœ¬
            item_text_chunks = text_splitter.split_text(item_text)
            
            chunk_seq_id = 0
            for chunk in item_text_chunks:
                form_name = file[file.rindex('/') + 1:file.rindex('.')]
                chunks_with_metadata.append({
                    'text': chunk,
                    'formItem': item,
                    'chunkSeqId': chunk_seq_id,
                    'chunkId': f'{form_name}-{item}-chunk{chunk_seq_id:04d}',
                    'source': file_as_object.get('Source', file),
                    'content_type': determine_content_type(item, chunk),
                    'industry': extract_industry_info(chunk),
                    'brand_mentioned': extract_brand_mentions(chunk)
                })
                chunk_seq_id += 1
            print(f'\tSplit into {chunk_seq_id} chunks')
        
        # ä¿å­˜åˆ†å—æ•°æ®
        output_dir = "data/chunks"
        os.makedirs(output_dir, exist_ok=True)
        
        filename = os.path.basename(file).replace('.json', '')
        output_file = os.path.join(output_dir, f"{filename}_chunks.json")
        
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(chunks_with_metadata, f, indent=2, ensure_ascii=False)
        
        print(f"Chunks saved to: {output_file}")
        return chunks_with_metadata
        
    except Exception as e:
        print(f"Error processing file {file}: {e}")
        return []

def determine_content_type(item, chunk):
    """ç¡®å®šå†…å®¹ç±»å‹"""
    content_type = "general"
    
    if 'brand' in item.lower() or 'å“ç‰Œ' in item:
        content_type = "brand_info"
    elif 'strategy' in item.lower() or 'ç­–ç•¥' in item:
        content_type = "strategy"
    elif 'campaign' in item.lower() or 'æ´»åŠ¨' in item:
        content_type = "campaign"
    elif 'media' in item.lower() or 'åª’ä½“' in item:
        content_type = "media"
    elif 'audience' in item.lower() or 'å—ä¼—' in item:
        content_type = "audience"
    elif 'result' in item.lower() or 'æ•ˆæœ' in item or 'kpi' in item.lower():
        content_type = "results"
    
    return content_type

def extract_industry_info(chunk):
    """æå–è¡Œä¸šä¿¡æ¯"""
    industries = ['ç§‘æŠ€', 'é‡‘è', 'é›¶å”®', 'æ±½è½¦', 'é£Ÿå“', 'æ—¶å°š', 'åŒ»ç–—', 'æ•™è‚²', 'æ—…æ¸¸']
    for industry in industries:
        if industry in chunk:
            return industry
    return "unknown"

def extract_brand_mentions(chunk):
    """æå–å“ç‰ŒæåŠ"""
    # ç®€å•çš„å“ç‰Œåç§°æå–ï¼ˆå¯ä»¥æ ¹æ®éœ€è¦æ‰©å±•ï¼‰
    brand_keywords = ['å“ç‰Œ', 'brand', 'å…¬å¸', 'ä¼ä¸š']
    brands = []
    
    for keyword in brand_keywords:
        if keyword in chunk.lower():
            # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´å¤æ‚çš„å“ç‰Œåç§°æå–é€»è¾‘
            brands.append(keyword)
    
    return brands

def process_all_pr_files(input_dir="data/json"):
    """å¤„ç†æ‰€æœ‰å…¬å…³ä¼ æ’­JSONæ–‡ä»¶"""
    input_path = Path(input_dir)
    
    if not input_path.exists():
        print(f"Input directory {input_dir} does not exist")
        return
    
    json_files = list(input_path.glob("*.json"))
    
    if not json_files:
        print(f"No JSON files found in {input_dir}")
        return
    
    print(f"Found {len(json_files)} JSON files to process")
    
    all_chunks = []
    
    for json_file in json_files:
        print(f"\nProcessing: {json_file.name}")
        chunks = split_pr_data_from_file(str(json_file))
        all_chunks.extend(chunks)
    
    print(f"\nâœ… å¤„ç†å®Œæˆï¼æ€»å…±ç”Ÿæˆäº† {len(all_chunks)} ä¸ªchunks")
    return all_chunks

if __name__ == "__main__":
    print("ğŸš€ å…¬å…³ä¼ æ’­å†…å®¹åˆ†å—å¼€å§‹")
    print("="*50)
    
    # å¤„ç†æ‰€æœ‰æ–‡ä»¶
    process_all_pr_files()
    
    print("\nâœ… åˆ†å—å®Œæˆï¼")
    print("å¤„ç†åçš„chunksä¿å­˜åœ¨ data/chunks/ ç›®å½•ä¸­")


