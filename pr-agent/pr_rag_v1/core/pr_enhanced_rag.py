#!/usr/bin/env python3
"""
å¢å¼ºçš„å…¬å…³ä¼ æ’­RAGç³»ç»Ÿ
åˆ©ç”¨å®ä½“å’Œå…³ç³»è¿›è¡Œæ›´ç²¾å‡†çš„æŸ¥è¯¢
"""

import textwrap
from typing import List, Dict, Any
from langchain_openai import ChatOpenAI
from langchain.prompts import PromptTemplate
from langchain_community.graphs import Neo4jGraph
from langchain_openai import OpenAIEmbeddings
from pr_neo4j_env import *

class EnhancedPRGraphRAG:
    """å¢å¼ºçš„å…¬å…³ä¼ æ’­GraphRAG"""
    
    def __init__(self):
        self.kg = Neo4jGraph(
            url=NEO4J_URI, 
            username=NEO4J_USERNAME, 
            password=NEO4J_PASSWORD, 
            database=NEO4J_DATABASE
        )
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            max_tokens=2000
        )
        
        # å¢å¼ºçš„CypheræŸ¥è¯¢æ¨¡æ¿
        self.cypher_query_template = PromptTemplate(
            input_variables=["question"],
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å…³ä¼ æ’­åˆ†æå¸ˆã€‚åŸºäºä»¥ä¸‹é—®é¢˜ï¼Œç”Ÿæˆç›¸åº”çš„CypheræŸ¥è¯¢è¯­å¥ã€‚

é—®é¢˜: {question}

å¯ç”¨çš„èŠ‚ç‚¹ç±»å‹:
- Brand: å“ç‰ŒèŠ‚ç‚¹ (name, industry, brand_positioning, brand_personality)
- Company: ä¼ä¸šèŠ‚ç‚¹ (name, industry, company_type, scale)
- Agency: å…¬å…³å…¬å¸èŠ‚ç‚¹ (name, specialization, service_scope)
- Campaign: ä¼ æ’­æ´»åŠ¨èŠ‚ç‚¹ (name, campaign_type, key_message, status)
- Media: åª’ä½“æ¸ é“èŠ‚ç‚¹ (name, media_type, reach, engagement_rate)
- Strategy: ä¼ æ’­ç­–ç•¥èŠ‚ç‚¹ (name, strategy_type, target_audience)
- PR_Chunk: æ–‡æœ¬åˆ†å—èŠ‚ç‚¹ (text, content_type, industry, brand_mentioned)

å¯ç”¨çš„å…³ç³»ç±»å‹:
- COLLABORATES_WITH: åˆä½œå…³ç³»
- BRAND_COLLABORATION: å“ç‰Œè”å
- MEDIA_PLACEMENT: åª’ä½“æŠ•æ”¾
- COMPETES_WITH: ç«äº‰å…³ç³»
- LAUNCHES_CAMPAIGN: å‘èµ·æ´»åŠ¨
- USES_STRATEGY: ä½¿ç”¨ç­–ç•¥
- CREATES_CONTENT: åˆ›å»ºå†…å®¹
- NEXT: æ–‡æœ¬é¡ºåºå…³ç³»

è¯·ç”Ÿæˆä¸€ä¸ªCypheræŸ¥è¯¢è¯­å¥æ¥å›ç­”è¿™ä¸ªé—®é¢˜ã€‚æŸ¥è¯¢åº”è¯¥:
1. ä¼˜å…ˆä½¿ç”¨å®ä½“èŠ‚ç‚¹å’Œå…³ç³»
2. å¦‚æœå®ä½“ä¿¡æ¯ä¸è¶³ï¼Œåˆ™æŸ¥è¯¢ç›¸å…³çš„PR_ChunkèŠ‚ç‚¹
3. è¿”å›æœ€ç›¸å…³çš„ä¿¡æ¯

åªè¿”å›CypheræŸ¥è¯¢è¯­å¥ï¼Œä¸è¦åŒ…å«å…¶ä»–è§£é‡Šã€‚
"""
        )

    def query(self, question: str) -> str:
        """æŸ¥è¯¢å¢å¼ºçš„å›¾è°±"""
        try:
            # ç”ŸæˆCypheræŸ¥è¯¢
            cypher_query = self._generate_cypher_query(question)
            
            # æ‰§è¡ŒæŸ¥è¯¢
            results = self.kg.query(cypher_query)
            
            # ç”Ÿæˆå›ç­”
            answer = self._generate_answer(question, results, cypher_query)
            
            return answer
            
        except Exception as e:
            return f"âŒ GraphRAGæŸ¥è¯¢å¤±è´¥: {e}"

    def _generate_cypher_query(self, question: str) -> str:
        """ç”ŸæˆCypheræŸ¥è¯¢è¯­å¥"""
        try:
            prompt = self.cypher_query_template.format(question=question)
            response = self.llm.invoke(prompt)
            return response.content.strip()
        except Exception as e:
            print(f"âš ï¸ CypheræŸ¥è¯¢ç”Ÿæˆå¤±è´¥: {e}")
            # å›é€€åˆ°ç®€å•æŸ¥è¯¢
            return self._fallback_cypher_query(question)

    def _fallback_cypher_query(self, question: str) -> str:
        """å›é€€çš„CypheræŸ¥è¯¢"""
        # ç®€å•çš„å…³é”®è¯åŒ¹é…æŸ¥è¯¢
        return """
        MATCH (pc:PR_Chunk)
        WHERE pc.text CONTAINS $keyword OR pc.brand_mentioned CONTAINS $keyword
        RETURN pc.text as text, pc.source as source, pc.brand_mentioned as brands
        ORDER BY pc.chunkSeqId
        LIMIT 5
        """

    def _generate_answer(self, question: str, results: List[Dict], cypher_query: str) -> str:
        """ç”Ÿæˆå›ç­”"""
        if not results:
            return "âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
        
        # æ„å»ºä¸Šä¸‹æ–‡
        context = self._build_context(results)
        
        # ç”Ÿæˆå›ç­”çš„æç¤º
        answer_prompt = f"""
åŸºäºä»¥ä¸‹å…¬å…³ä¼ æ’­çŸ¥è¯†å›¾è°±çš„æŸ¥è¯¢ç»“æœï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

æŸ¥è¯¢ç»“æœ:
{context}

è¯·åŸºäºè¿™äº›ä¿¡æ¯æä¾›ä¸€ä¸ªä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ã€‚å›ç­”åº”è¯¥:
1. ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
2. å¼•ç”¨å…·ä½“çš„å“ç‰Œã€ä¼ä¸šã€æ´»åŠ¨æˆ–ç­–ç•¥
3. æä¾›å®ç”¨çš„å»ºè®®æˆ–æ´å¯Ÿ
4. ä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§

å›ç­”:
"""
        
        try:
            response = self.llm.invoke(answer_prompt)
            return response.content
        except Exception as e:
            return f"âŒ å›ç­”ç”Ÿæˆå¤±è´¥: {e}"

    def _build_context(self, results: List[Dict]) -> str:
        """æ„å»ºä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, result in enumerate(results[:5]):  # é™åˆ¶ç»“æœæ•°é‡
            context_part = f"ç»“æœ {i+1}:\n"
            
            # å¤„ç†ä¸åŒç±»å‹çš„ç»“æœ
            if 'text' in result:
                context_part += f"å†…å®¹: {result['text'][:200]}...\n"
            if 'source' in result:
                context_part += f"æ¥æº: {result['source']}\n"
            if 'brands' in result:
                context_part += f"ç›¸å…³å“ç‰Œ: {result['brands']}\n"
            if 'name' in result:
                context_part += f"å®ä½“åç§°: {result['name']}\n"
            if 'industry' in result:
                context_part += f"è¡Œä¸š: {result['industry']}\n"
            if 'description' in result:
                context_part += f"æè¿°: {result['description']}\n"
            
            context_parts.append(context_part)
        
        return "\n".join(context_parts)

class EnhancedPRVectorRAG:
    """å¢å¼ºçš„å…¬å…³ä¼ æ’­VectorRAG"""
    
    def __init__(self):
        self.kg = Neo4jGraph(
            url=NEO4J_URI, 
            username=NEO4J_USERNAME, 
            password=NEO4J_PASSWORD, 
            database=NEO4J_DATABASE
        )
        self.llm = ChatOpenAI(
            model="gpt-3.5-turbo",
            temperature=0.1,
            max_tokens=2000
        )
        self.embeddings = OpenAIEmbeddings()
        
        # å¢å¼ºçš„å‘é‡æŸ¥è¯¢æ¨¡æ¿
        self.vector_query_template = PromptTemplate(
            input_variables=["question", "context"],
            template="""
ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å…¬å…³ä¼ æ’­åˆ†æå¸ˆã€‚åŸºäºä»¥ä¸‹ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚

ç”¨æˆ·é—®é¢˜: {question}

ä¸Šä¸‹æ–‡ä¿¡æ¯:
{context}

è¯·æä¾›ä¸€ä¸ªä¸“ä¸šã€å‡†ç¡®çš„å›ç­”ï¼ŒåŒ…æ‹¬:
1. ç›´æ¥å›ç­”ç”¨æˆ·çš„é—®é¢˜
2. å¼•ç”¨å…·ä½“çš„æ¡ˆä¾‹ã€å“ç‰Œæˆ–ç­–ç•¥
3. æä¾›å®ç”¨çš„å»ºè®®
4. ä¿æŒä¸“ä¸šæ€§å’Œå‡†ç¡®æ€§

å›ç­”:
"""
        )

    def query(self, question: str) -> str:
        """æŸ¥è¯¢å¢å¼ºçš„å‘é‡ç´¢å¼•"""
        try:
            # ç”Ÿæˆé—®é¢˜åµŒå…¥
            question_embedding = self.embeddings.embed_query(question)
            
            # å‘é‡ç›¸ä¼¼æ€§æŸ¥è¯¢
            vector_query = f"""
            CALL db.index.vector.queryNodes('{VECTOR_INDEX_NAME}', 5, $embedding)
            YIELD node, score
            RETURN node.text as text, 
                   node.source as source, 
                   node.content_type as content_type,
                   node.industry as industry,
                   node.brand_mentioned as brand_mentioned,
                   score
            ORDER BY score DESC
            """
            
            results = self.kg.query(vector_query, params={'embedding': question_embedding})
            
            if not results:
                return "âŒ æœªæ‰¾åˆ°ç›¸å…³ä¿¡æ¯"
            
            # æ„å»ºä¸Šä¸‹æ–‡
            context = self._build_vector_context(results)
            
            # ç”Ÿæˆå›ç­”
            answer_prompt = self.vector_query_template.format(question=question, context=context)
            response = self.llm.invoke(answer_prompt)
            
            return response.content
            
        except Exception as e:
            return f"âŒ VectorRAGæŸ¥è¯¢å¤±è´¥: {e}"

    def _build_vector_context(self, results: List[Dict]) -> str:
        """æ„å»ºå‘é‡æŸ¥è¯¢ä¸Šä¸‹æ–‡"""
        context_parts = []
        
        for i, result in enumerate(results):
            context_part = f"ç›¸å…³æ–‡æ¡£ {i+1} (ç›¸ä¼¼åº¦: {result['score']:.3f}):\n"
            context_part += f"å†…å®¹: {result['text'][:300]}...\n"
            context_part += f"æ¥æº: {result['source']}\n"
            context_part += f"å†…å®¹ç±»å‹: {result['content_type']}\n"
            context_part += f"è¡Œä¸š: {result['industry']}\n"
            context_part += f"ç›¸å…³å“ç‰Œ: {result['brand_mentioned']}\n"
            context_parts.append(context_part)
        
        return "\n".join(context_parts)

class EnhancedPRRAGSystem:
    """å¢å¼ºçš„å…¬å…³ä¼ æ’­RAGç³»ç»Ÿ"""
    
    def __init__(self):
        self.graph_rag = EnhancedPRGraphRAG()
        self.vector_rag = EnhancedPRVectorRAG()
        
    def query(self, question: str, use_graph: bool = True) -> str:
        """æŸ¥è¯¢å¢å¼ºçš„RAGç³»ç»Ÿ"""
        print(f"ğŸ” æŸ¥è¯¢é—®é¢˜: {question}")
        print(f"ğŸ“Š ä½¿ç”¨æ¨¡å¼: {'GraphRAG' if use_graph else 'VectorRAG'}")
        print("-" * 60)
        
        if use_graph:
            return self.graph_rag.query(question)
        else:
            return self.vector_rag.query(question)
    
    def get_entity_relationships(self, entity_name: str) -> Dict[str, Any]:
        """è·å–å®ä½“çš„å…³ç³»ä¿¡æ¯"""
        try:
            # æŸ¥è¯¢å®ä½“åŠå…¶å…³ç³»
            entity_query = """
            MATCH (e)-[r]->(related)
            WHERE e.name CONTAINS $entity_name
            RETURN e.name as entity_name, 
                   type(r) as relationship_type, 
                   related.name as related_entity,
                   labels(related) as related_type
            LIMIT 20
            """
            
            results = self.graph_rag.kg.query(entity_query, params={'entity_name': entity_name})
            
            # ç»„ç»‡å…³ç³»æ•°æ®
            relationships = {
                'entity_name': entity_name,
                'outgoing_relationships': [],
                'incoming_relationships': []
            }
            
            for result in results:
                rel_info = {
                    'type': result['relationship_type'],
                    'related_entity': result['related_entity'],
                    'related_type': result['related_type']
                }
                relationships['outgoing_relationships'].append(rel_info)
            
            return relationships
            
        except Exception as e:
            return {'error': f"è·å–å®ä½“å…³ç³»å¤±è´¥: {e}"}
    
    def get_brand_collaborations(self, brand_name: str) -> List[Dict[str, Any]]:
        """è·å–å“ç‰Œåˆä½œå…³ç³»"""
        try:
            collaboration_query = """
            MATCH (b:Brand)-[r:BRAND_COLLABORATION|COLLABORATES_WITH]->(related)
            WHERE b.name CONTAINS $brand_name
            RETURN b.name as brand_name,
                   type(r) as collaboration_type,
                   related.name as partner_name,
                   labels(related) as partner_type,
                   r.description as description
            """
            
            results = self.graph_rag.kg.query(collaboration_query, params={'brand_name': brand_name})
            
            collaborations = []
            for result in results:
                collaborations.append({
                    'brand_name': result['brand_name'],
                    'collaboration_type': result['collaboration_type'],
                    'partner_name': result['partner_name'],
                    'partner_type': result['partner_type'],
                    'description': result['description']
                })
            
            return collaborations
            
        except Exception as e:
            return [{'error': f"è·å–å“ç‰Œåˆä½œå…³ç³»å¤±è´¥: {e}"}]
    
    def get_media_strategies(self, brand_name: str) -> List[Dict[str, Any]]:
        """è·å–å“ç‰Œçš„åª’ä½“ç­–ç•¥"""
        try:
            media_strategy_query = """
            MATCH (b:Brand)-[r:MEDIA_PLACEMENT]->(m:Media)
            WHERE b.name CONTAINS $brand_name
            RETURN b.name as brand_name,
                   m.name as media_name,
                   m.media_type as media_type,
                   m.reach as reach,
                   r.description as strategy_description
            """
            
            results = self.graph_rag.kg.query(media_strategy_query, params={'brand_name': brand_name})
            
            strategies = []
            for result in results:
                strategies.append({
                    'brand_name': result['brand_name'],
                    'media_name': result['media_name'],
                    'media_type': result['media_type'],
                    'reach': result['reach'],
                    'strategy_description': result['strategy_description']
                })
            
            return strategies
            
        except Exception as e:
            return [{'error': f"è·å–åª’ä½“ç­–ç•¥å¤±è´¥: {e}"}]

def test_enhanced_rag():
    """æµ‹è¯•å¢å¼ºçš„RAGç³»ç»Ÿ"""
    rag_system = EnhancedPRRAGSystem()
    
    test_questions = [
        "åä¸åæœ‰å“ªäº›å“ç‰Œåˆä½œæ¡ˆä¾‹ï¼Ÿ",
        "å°ç±³åœ¨å“ªäº›åª’ä½“å¹³å°è¿›è¡Œæ¨å¹¿ï¼Ÿ",
        "å¥¥è¿ªçš„å“ç‰Œä¼ æ’­ç­–ç•¥æ˜¯ä»€ä¹ˆï¼Ÿ",
        "æ±½è½¦è¡Œä¸šçš„å…¬å…³ä¼ æ’­æœ‰ä»€ä¹ˆç‰¹ç‚¹ï¼Ÿ"
    ]
    
    print("ğŸ§ª æµ‹è¯•å¢å¼ºçš„RAGç³»ç»Ÿ")
    print("=" * 60)
    
    for question in test_questions:
        print(f"\nğŸ¤” é—®é¢˜: {question}")
        print("-" * 40)
        
        # æµ‹è¯•GraphRAG
        print("ğŸ“Š GraphRAGå›ç­”:")
        graph_answer = rag_system.query(question, use_graph=True)
        print(textwrap.fill(graph_answer, 80))
        
        print("\n" + "-" * 40)
        
        # æµ‹è¯•VectorRAG
        print("ğŸ” VectorRAGå›ç­”:")
        vector_answer = rag_system.query(question, use_graph=False)
        print(textwrap.fill(vector_answer, 80))
        
        print("\n" + "=" * 60)

if __name__ == "__main__":
    test_enhanced_rag()
